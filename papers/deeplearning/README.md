# Deep Learning

## Papers Information

- **Title**: Deep Learning
- **Authors**: Yann Lecunn, Yoshua Bengio, Geoffrey Hinton
- **Year**: 2015
- **Conference/Journal**: Nature
- **URL**: [Deep Learning](https://www.nature.com/articles/nature14539)

## Key Contributions
- Historical Overview of Deep Learning
    - Rosenblatt introduces perceptron (1958)
    - Minsky and Papert show limitations of perceptron (1969)
    - Werbos introduces backpropagation to train MLPs (1974)
    - Rumelhart et al. popularize backpropagation (1986)
    - LeCun et al. introduce convolutional neural networks (1989)
    - Hochreiter and Schmidhuber introduce long short-term memory (1997) 
    - Hinton et al. introduce deep belief networks (2006)
    - Krizhevsky et al. win ImageNet competition with deep convolutional neural network (2012)
    - Mikolov et al. introduce word2vec (2013)

## Discussion Points
- What are the advantages of deep learning over traditional machine learning methods?
- What are the key reasons behind the recent success of deep learning?
    - Large datasets
    - Powerful GPUs and bettet compute 
- What are filter banks? 
    - Multiple kernels 
- Describe backapropagation and stochastic gradient descent
- Why was backpropagation forsaken by the machine learning community in the 1990s?
- What are the key ideas behind convulutional neural networks?
- What are the key ideas behind recurrent neural networks?


## Conclusions
[Provide a concluding statement or overall thoughts on the paper.]

## Additional Resources

- [Include any additional resources or related papers.]
